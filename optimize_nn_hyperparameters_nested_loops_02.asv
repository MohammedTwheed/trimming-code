%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MAIN
clear; clc; clf; close all;
load('training-data/filtered_QHD_table.mat')
load('training-data/filtered_QDP_table.mat')
load('training-data/deleted_QHD_table.mat')
load('training-data/deleted_QDP_table.mat')

QH = [filtered_QHD_table.FlowRate_m3h,filtered_QHD_table.Head_m]';
D  = [filtered_QHD_table.Diameter_mm]';

QH_beps=[deleted_QHD_table.FlowRate_m3h,deleted_QHD_table.Head_m]';
D_beps=[deleted_QHD_table.Diameter_mm]';

QD = [filtered_QDP_table.FlowRate_m3h,filtered_QDP_table.Diameter_mm]';
P = [filtered_QDP_table.Power_kW]';

QD_beps=[deleted_QDP_table.FlowRate_m3h,deleted_QDP_table.Diameter_mm]';
P_beps=[deleted_QDP_table.Power_kW]';

% User-specified random seed (optional)
% Replace with your desired seed (or leave empty)
userSeed = 4826;

% Define a threshold for MSE to exit the loop early
mseThreshold = 0.00199;

% Find all distinct diameters in D
distinctDiameters = unique(D);

% Weights for combining MSEs
weightDiameter = 0.5;
weightBeps = 0.5;

structFilename = './optimize_nn_hyperparameters_nested_loops_02/results_struct.mat';



% Initialize an empty struct array
resultsStruct = struct('Timestamp', {}, 'DiameterRemoved', {}, 'Iteration', {}, 'HiddenLayersSizes', {}, ...
    'MaxEpochs', {}, 'TrainingFunction', {}, 'ActivationFunction', {}, 'FinalMSE', {}, 'RandomSeed', {}, 'TrainingError', {}, ...
    'ValidationError', {}, 'TestError',{},'mseQH_beps', {}, 'TrainedNet', {}, 'FigureFileName', {});


for dIdx = 1:length(distinctDiameters)
    % Current diameter to remove
    diameterToRemove = distinctDiameters(dIdx);
    
    % Find indices of the current diameter in D
    indicesToRemove = find(QD(:,2) == diameterToRemove);
    
    % Store the removed data for later use
    removedQD = QD(:, indicesToRemove);
    removedP  = P(indicesToRemove);
    
    % Remove rows from QH and D based on the indices
    QD_temp = QD;
    P_temp  = P;
    QD_temp(:, indicesToRemove) = [];
    P_temp(:, indicesToRemove)  = [];
    
    Qa = QD_temp(1,:);
    Ha = QD_temp(2,:);
    Q  = QD_temp(1,:);
    H  = QD_temp(2,:);

    % Initialize bounds
    lower_bounds=[2,    9,  12,   100, 1, 1];
    upper_bounds=[17,  95,  20,  300, 1, 1];
    
    % Track the previous combined MSE to determine the improvement
    prevCombinedMSE = inf;

    for i = 1:30
        [optimalHyperParamsH, finalMSEH, randomSeedH, bestTrainedNetH, error] = ...
            optimizeNNForTrimmingPumpImpeller([QH_temp(1,:); D_temp], QH_temp(2,:), userSeed+i, lower_bounds, upper_bounds);

        % Calculate MSE for the removed diameter
        predictedH = bestTrainedNetH([removedQH(1, :); removedD])';
        mseDiameter = mean((removedQH(2, :)' - predictedH).^2 / sum(removedQH(2, :)));

        predictedH_beps = bestTrainedNetH([QH_beps(1,:); D_beps])';
        mseQH_beps = mean((QH_beps(2,:)' - predictedH_beps).^2 / sum(QH_beps(2,:)));

        fprintf('Diameter %d, Iteration %d, MSE_Dia: %.6f,  MSE_beps: %.6f \n', diameterToRemove, i, mseDiameter, mseQH_beps);



        % Define desired diameter values 
        desiredDiameters = distinctDiameters; 

        % Create a single figure for all plots
        figure;
        hold on;  % Keep plots on the same figure

        % Plot the removed diameter data points
        scatter(removedQH(1, :)', removedQH(2, :)', 'r', 'filled', 'DisplayName', sprintf('Removed Diameter: %dmm', diameterToRemove));
        
        Qt = linspace(0, 400, 200);

        for diameterIndex = 1:length(desiredDiameters)
            desiredDiameter = desiredDiameters(diameterIndex);
            Dt = repmat(desiredDiameter, length(Qt), 1);

            filteredQH = bestTrainedNetH([Qt; Dt'])';

            % Define legend entry for each diameter
            legendLabel = strcat('Diameter: ', num2str(desiredDiameter), 'mm');

            % Plot Q vs H with appropriate label and legend entry
            plot(Qt, filteredQH, 'DisplayName', legendLabel);
            
            % Add a callout marker at the end of the curve
            text(Qt(end), filteredQH(end), sprintf('%dmm', desiredDiameter), 'FontSize', 8, 'Color', 'black', 'BackgroundColor', 'white');
        end

        % Plot the remaining original data points
        scatter(Qa', Ha', 'b', 'filled', 'DisplayName', 'Reference Points');
        
        xlabel('Q (m^3/h)');
        ylabel('H (m)');
        title(['(Q, H) slices with Diameters, Removed Diameter: ' num2str(diameterToRemove) 'mm']);
        legend;  
        hold off;

% Save the plot with a descriptive filename
filename = sprintf('./optimize_nn_hyperparameters_nested_loops_02/nn_diameter-%d_iteration_%d_mseDia-%.6f_test-%.6f_mseQH_beps-%6f.png', diameterToRemove, i, mseDiameter, error(3), mseQH_beps);
saveas(gcf, filename);

% Determine the number of hidden layers
numHiddenLayers = length(optimalHyperParamsH) - 3;

% Extract hyperparameters
hiddenLayersSizes = optimalHyperParamsH(1:numHiddenLayers);
maxEpochs = optimalHyperParamsH(end - 2);
trainingFunctionIdx = optimalHyperParamsH(end - 1);
activationFunctionIdx = optimalHyperParamsH(end);

% Create a new entry in the results struct
newEntry = struct(...
    'Timestamp', datetime('now', 'Format', 'yyyy-MM-dd HH:mm:ss'), ...
    'DiameterRemoved', diameterToRemove, ...
    'Iteration', i, ...
    'HiddenLayersSizes', hiddenLayersSizes, ...
    'MaxEpochs', maxEpochs, ...
    'TrainingFunction', trainingFunctionIdx, ...
    'ActivationFunction', activationFunctionIdx, ...
    'FinalMSE', finalMSEH, ...
    'RandomSeed', randomSeedH, ...
    'TrainingError', error(1), ...
    'ValidationError', error(2), ...
    'TestError', error(3), ...
    'mseQH_beps', mseQH_beps, ...
    'TrainedNet', bestTrainedNetH, ...
    'FigureFileName', filename);

% Specify the filename for saving the network
filename = sprintf('./optimize_nn_hyperparameters_nested_loops_02/nn_diameter-%d_iteration_%d_mseDia-%.6f_test-%.6f_mseQH_beps-%6f.mat', diameterToRemove, i, mseDiameter, error(3), mseQH_beps);

        % Save the network to the .mat file
        save(filename, 'bestTrainedNetH');


        % Append the new entry to the results struct array
        resultsStruct(end+1) = newEntry;

        % Save the struct array to a .mat file
        save(structFilename, 'resultsStruct');
        % Close the figure to avoid memory issues
        close(gcf);



        % Exit loop if MSE is below the threshold
        if (mseDiameter < mseThreshold) && (error(3) < 0.0199) && (mseQH_beps < mseThreshold)
            fprintf('MSE for diameter %d is below the threshold. Exiting loop.\n', diameterToRemove);
            break;
        end
    end
end

disp('Results saved to optimize_nn_hyperparameters_nested_loops_02/results_loop.csv');

% END MAIN
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%








function [QH, D, QD, P] = loadData(dataPath)
% loadData: Loads data from files into MATLAB variables. Inputs:
%   dataPath - Path to the directory containing the data files.
% Outputs:
%   QH - Q flowrate and Head data (corresponding to D diameters).
%   (matrix) D - Diameters. (matrix) QD - Q flowrate and diameters
%   (corresponding to power in P). (matrix) P - Power values. (matrix)

% Validate data path
if ~exist(dataPath, 'dir')
    error('Data directory does not exist: %s', dataPath);
end

% Load data from files with error handling
try
    QH = load(fullfile(dataPath, 'QH.mat'));
    D = load(fullfile(dataPath, 'D.mat'));
    QD = load(fullfile(dataPath, 'QD.mat'));
    P = load(fullfile(dataPath, 'Pow.mat'));
catch ME
    error('Error loading data: %s', ME.message);
end

% Extract desired variables directly (note that this according to our own
% data only)
QH = transpose(QH.QH); %  QH struct contains a single variable named 'QH'
D = transpose(D.D);    %  D struct contains a single variable named 'D'
QD = transpose(QD.QD); %  QD struct contains a single variable named 'QD'
P = transpose(P.P);    %  P struct contains a single variable named 'P'

% the transpose here b.c the of the way train() function in matlab
% interpret the input output featrues see documentation for more info
% without transpose it would treat the whole vector or matrix as one
% input feature.
end





function [optimalHyperParams, finalMSE, randomSeed, bestTrainedNet, nnPerfVect] = optimizeNNForTrimmingPumpImpeller(x, t, userSeed, lowerBounds, upperBounds)
% optimizeNNForTrimmingPumpImpeller: Optimizes neural network
% hyperparameters for pump impeller trimming. Inputs:
%   x - Input data (Q flowrate, H head) for the neural network.
%   t - Target data (D diameter or power but not both at same time) for the neural network.
%   userSeed (optional) - User-specified random seed for reproducibility.
% Outputs:
%   optimalHyperParams - Optimized hyperparameters found by the genetic algorithm.
%   finalMSE - Mean squared error (MSE) of the best model.
%   randomSeed - Random seed used for reproducibility (user-specified or generated).
%   bestTrainedNet - The best trained neural network found during optimization.

% Set random seed (user-specified or generated)
if nargin < 3
    randomSeed = randi(10000);
else
    randomSeed = userSeed;
end

% Start timer to measure the duration of the optimization process.
tic;
disp("Optimization of nn in progress. This process may take up to 1 or 2 hours using my default settings...");

% Define the available options for training functions and activation functions.
trainingFunctionOptions = {'trainlm', 'trainbr', 'trainrp', 'traincgb', 'traincgf', 'traincgp', 'traingdx', 'trainoss'};
activationFunctionOptions = {'tansig', 'logsig'};

% Define options for the genetic algorithm.
gaOptions = optimoptions('ga', ...
    'PopulationSize', 17, ...
    'MaxGenerations', 13, ...
    'CrossoverFraction', 0.8, ...
    'ConstraintTolerance', 0.000991, ...
    'FitnessLimit', 0.000991, ...
    'EliteCount', 2, ...
    'Display', 'iter', ...
    'UseParallel', true);

% Global variable to store the best trained neural network found during optimization.
global bestTrainedNet;
bestTrainedNet = [];
global nnPerfVect;
nnPerfVect = [];

% Local function to evaluate hyperparameters using the neural network.
function [avgMSEs] = evaluateHyperparameters(hyperParams, x, t, randomSeed)
    rng(randomSeed); % Set random seed for reproducibility.

    % Determine the number of hidden layers
    numHiddenLayers = length(hyperParams) - 3;

    % Extract hyperparameters
    hiddenLayersSizes = round(hyperParams(1:numHiddenLayers)); % Hidden Layers Sizes
    maxEpochs = round(hyperParams(end - 2));                   % Max Epochs
    trainingFunctionIdx = round(hyperParams(end - 1));         % Training Function
    activationFunctionIdx = round(hyperParams(end));           % Activation Function

    % Define the neural network with dynamic hidden layers.
    net = feedforwardnet(hiddenLayersSizes, trainingFunctionOptions{trainingFunctionIdx});

    % Suppress training GUI for efficiency.
    net.trainParam.showWindow = false;
    net.trainParam.epochs = maxEpochs;

    % Set the transfer function for each hidden layer.
    for i = 1:numHiddenLayers
        net.layers{i}.transferFcn = activationFunctionOptions{activationFunctionIdx};
    end

    % Choose a Performance Function
    net.performFcn = 'mse';

    % Choose Input and Output Pre/Post-Processing Functions
    net.input.processFcns = {'removeconstantrows', 'mapminmax'};
    net.output.processFcns = {'removeconstantrows', 'mapminmax'};

    % Define data split for training, validation, and testing.
    net.divideFcn = 'dividerand';  % Divide data randomly
    net.divideMode = 'sample';  % Divide up every sample
    net.divideParam.trainRatio = 0.7;
    net.divideParam.valRatio = 0.15;
    net.divideParam.testRatio = 0.15;

    % Train the neural network.
    [trainedNet, tr] = train(net, x, t);

    % Evaluate the model performance using mean squared error (MSE).
    predictions = trainedNet(x);
    mse = perform(trainedNet, t, predictions);

    % Recalculate Training, Validation and Test Performance
    trainTargets = t .* tr.trainMask{1};
    valTargets = t .* tr.valMask{1};
    testTargets = t .* tr.testMask{1};
    trainPerformance = perform(net, trainTargets, predictions);
    valPerformance = perform(net, valTargets, predictions);
    testPerformance = perform(net, testTargets, predictions);

    % Calculate the average MSE
    avgMSEs = (mse + trainPerformance + valPerformance + testPerformance) / 4;

    % Check if the current MSE is the best MSE so far and update the global variable if necessary.
    if isempty(bestTrainedNet) || avgMSEs < perform(bestTrainedNet, t, bestTrainedNet(x))
        bestTrainedNet = trainedNet;
        nnPerfVect = [trainPerformance, valPerformance, testPerformance];
    end
end

% Set a random seed for reproducibility.
rng(randomSeed);

% Perform optimization using genetic algorithm.
[optimalHyperParams, finalMSE] = ga(@(hyperParams)evaluateHyperparameters(hyperParams, x, t, randomSeed), ...
    length(lowerBounds), [], [], [], [], lowerBounds, upperBounds, [], gaOptions);

% Round the optimized hyperparameters to integers.
optimalHyperParams = round(optimalHyperParams);

% Measure elapsed time.
elapsedTime = toc;

% Extract the chosen training and activation functions.
trainingFunction = trainingFunctionOptions{optimalHyperParams(end - 1)};
activationFunction = activationFunctionOptions{optimalHyperParams(end)};

% Logging functionality
logFile = 'optimizeNNForTrimmingPumpImpeller_log.txt';

% Open log file for appending (create if it doesn't exist)
fid = fopen(logFile, 'a');
if fid == -1
    error('Error opening log file for appending.');
end

% Write current timestamp to log file
currentTime = datetime('now', 'Format', 'yyyy-MM-dd HH:MM:SS');
fprintf(fid, '%s\n', char(currentTime));

% Write optimization results to log file
fprintf(fid, 'Optimized Hyperparameters: Hidden Layers Sizes = %s, Max Epochs = %d, Training Function = %s, Activation Function = %s\n', ...
    mat2str(optimalHyperParams(1:end-3)), optimalHyperParams(end-2), trainingFunction, activationFunction);
fprintf(fid, 'Final Mean Squared Error (MSE): %.4f\n', finalMSE);
fprintf(fid, 'Random Seed Used: %d\n', randomSeed);
fprintf(fid, 'Optimization Duration: %.4f seconds\n\n', elapsedTime);

% Close the log file
fclose(fid);

% Display results
fprintf('Optimized Hyperparameters: Hidden Layers Sizes = %s, Max Epochs = %d, Training Function = %s, Activation Function = %s\n', ...
    mat2str(optimalHyperParams(1:end-3)), optimalHyperParams(end-2), trainingFunction, activationFunction);
fprintf('Final Mean Squared Error (MSE): %.4f\n', finalMSE);
fprintf('Random Seed Used: %d\n', randomSeed);
fprintf('Optimization Duration: %.4f seconds\n', elapsedTime);

end


